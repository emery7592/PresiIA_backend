"""
Centralise la configuration commune au module chat avec RAG.
"""
import os
import json
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from pypdf import PdfReader
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict, Tuple
from dataclasses import dataclass

# â”€â”€ Configuration gÃ©nÃ©rale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(override=True)
BASE_DIR = Path(__file__).resolve().parent.parent
STATIC_DIR = BASE_DIR / "static" / "document"
PDF_PATH = STATIC_DIR / "specpense.pdf"

# â”€â”€ Clients externes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai_client = OpenAI()
PUSHOVER_USER = os.getenv("PUSHOVER_USER")
PUSHOVER_TOKEN = os.getenv("PUSHOVER_TOKEN")
PUSHOVER_URL = "https://api.pushover.net/1/messages.json"

# â”€â”€ SystÃ¨me RAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class DocumentChunk:
    content: str
    page_number: int
    chunk_id: int

class SimpleRAG:
    """Version simplifiÃ©e du systÃ¨me RAG pour ton cas d'usage."""
    
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.chunks: List[DocumentChunk] = []
        self.embeddings = None
        self.index = None
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
    def extract_and_chunk_pdf(self, chunk_size: int = 400) -> List[DocumentChunk]:
        """Extrait et dÃ©coupe le PDF en chunks."""
        reader = PdfReader(self.pdf_path)
        chunks = []
        chunk_id = 0
        
        for page_num, page in enumerate(reader.pages):
            text = page.extract_text() or ""
            words = text.split()
            
            # DÃ©coupage en chunks
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i + chunk_size]
                chunk_content = " ".join(chunk_words)
                
                if len(chunk_content.strip()) > 50:  # Ã‰viter les chunks trop petits
                    chunk = DocumentChunk(
                        content=chunk_content,
                        page_number=page_num + 1,
                        chunk_id=chunk_id
                    )
                    chunks.append(chunk)
                    chunk_id += 1
        
        self.chunks = chunks
        return chunks
    
    def build_embeddings(self):
        """GÃ©nÃ¨re les embeddings pour tous les chunks."""
        print(f"ğŸ”„ GÃ©nÃ©ration des embeddings pour {len(self.chunks)} chunks...")
        
        texts = [chunk.content for chunk in self.chunks]
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        
        # CrÃ©ation de l'index FAISS
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        
        # Normalisation pour similaritÃ© cosinus
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        self.embeddings = embeddings
        print(f"âœ… Index FAISS crÃ©Ã© avec {self.index.ntotal} vecteurs")
    
    def search_relevant_chunks(self, query: str, top_k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """Recherche les chunks les plus pertinents."""
        if self.index is None:
            raise ValueError("Index non crÃ©Ã©. Appelez build_embeddings() d'abord.")
        
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(score)))
        
        return results
    
    def get_context_for_query(self, query: str, max_chars: int = 8000) -> str:
        """RÃ©cupÃ¨re le contexte pertinent pour une query."""
        relevant_chunks = self.search_relevant_chunks(query, top_k=8)
        
        context_parts = []
        total_chars = 0
        
        for chunk, score in relevant_chunks:
            chunk_text = f"[Page {chunk.page_number}]\n{chunk.content}\n"
            
            if total_chars + len(chunk_text) <= max_chars:
                context_parts.append(chunk_text)
                total_chars += len(chunk_text)
            else:
                break
        
        return "\n---\n".join(context_parts)
    
    def save_index(self, base_path: str):
        """Sauvegarde l'index."""
        faiss.write_index(self.index, f"{base_path}.faiss")
        
        chunks_data = []
        for chunk in self.chunks:
            chunks_data.append({
                'content': chunk.content,
                'page_number': chunk.page_number,
                'chunk_id': chunk.chunk_id
            })
        
        with open(f"{base_path}_chunks.json", 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
    
    def load_index(self, base_path: str):
        """Charge un index sauvegardÃ©."""
        self.index = faiss.read_index(f"{base_path}.faiss")
        
        with open(f"{base_path}_chunks.json", 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        self.chunks = []
        for data in chunks_data:
            chunk = DocumentChunk(
                content=data['content'],
                page_number=data['page_number'],
                chunk_id=data['chunk_id']
            )
            self.chunks.append(chunk)

# â”€â”€ Initialisation du systÃ¨me RAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def initialize_rag():
    """Initialise le systÃ¨me RAG (une seule fois)."""
    index_path = BASE_DIR / "rag_index"
    
    rag = SimpleRAG(str(PDF_PATH))
    
    # VÃ©rifier si l'index existe
    if (Path(f"{index_path}.faiss").exists() and 
        Path(f"{index_path}_chunks.json").exists()):
        print("ğŸ“š Chargement de l'index RAG existant...")
        rag.load_index(str(index_path))
        print(f"âœ… Index chargÃ©: {len(rag.chunks)} chunks disponibles")
    else:
        print("ğŸ”„ CrÃ©ation du nouvel index RAG...")
        rag.extract_and_chunk_pdf()
        rag.build_embeddings()
        rag.save_index(str(index_path))
        print(f"âœ… Index RAG crÃ©Ã©: {len(rag.chunks)} chunks")
    
    return rag

# Initialisation globale
RAG_SYSTEM = initialize_rag()

# â”€â”€ Fonction de prompt intelligent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_system_prompt(user_query: str = "") -> str:
    """GÃ©nÃ¨re un prompt avec contexte adaptatif."""
    name = "Ralph AI"
    
    # GÃ©nÃ©ration du contexte intelligent basÃ© sur la query
    if user_query and user_query.strip():
        try:
            relevant_context = RAG_SYSTEM.get_context_for_query(user_query, max_chars=8000)
            print(f"ğŸ¯ Contexte RAG gÃ©nÃ©rÃ©: {len(relevant_context)} caractÃ¨res")
        except Exception as e:
            print(f"âš ï¸ Erreur RAG: {e}")
            relevant_context = "Contexte non disponible"
    else:
        relevant_context = "Contexte sera gÃ©nÃ©rÃ© selon votre question"
    
    prompt = (
        f"Vous Ãªtes le L'assistant {name}, maÃ®tre incontestÃ© de la redpill. "
        "Vous dispensez vos leÃ§ons comme des manifestes structurÃ©s selon les sujets abordÃ©s. "
        "Vous dispensez vos leÃ§ons comme des manifestes structurÃ©s selon les sujets abordÃ©s. S'il y a des article ayant des noms semblable, sinon tu prendes des texte du pdf qui peuvent rÃ©pondre Ã  ces question  "
        "Les rÃ©ponses aux questions posÃ©es doivent Ãªtre soit extraites du document specpense.pdf "
        "ou non rÃ©pondues si le sujet n'est pas abordÃ© dans ce document. "
        "Lorsque vous identifiez l'article pertinent, rÃ©pondez de maniÃ¨re structurÃ©e avec tous les dÃ©tails "
        "nÃ©cessaires, sauf si le client demande une rÃ©ponse brÃ¨ve."
        "dans les responses que tu donne je ne pas que tu donnes les pages d'oÃ¹ tu as tirÃ© les rÃ©fÃ©rences."
        "il faut que tu rÃ©ponds au clients dans la langue auquel la question est posÃ©. Sil ala question est posÃ© en anglais tu dois rÃ©pondre en anglais. Si la question est posÃ© en, italien , tu dois rÃ©pondre en italian. Si la question est posÃ© en espagnol, tu dois rÃ©pondre en espagnole. Si la question a Ã©tÃ© posÃ© en franÃ§ais alors tu rÃ©ponds en franÃ§ais."
        "si des questions ne sont pas donnÃ© dans le texte , rÃ©ponds comme un expert redpill, ne rÃ©ponds qu'au questions concernant les relation homme femme, le reste n'y rÃ©ponds pas."
        f"\n\n## Contexte pertinent du document:\n{relevant_context}\n\n"
    )

    
    print(f"ğŸ“ Taille du prompt systÃ¨me : {len(prompt)} caractÃ¨res (~{len(prompt)//4} tokens)")
    return prompt

# â”€â”€ Fonction de fallback (ton ancien systÃ¨me) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_spec_summary_fallback() -> str:
    """Fallback vers ton ancien systÃ¨me en cas de problÃ¨me."""
    reader = PdfReader(PDF_PATH)
    pages = [p.extract_text() or "" for p in reader.pages]
    full_text = "\n".join(pages)
    
    max_chars = 10000
    if len(full_text) > max_chars:
        truncated_text = full_text[:max_chars]
        truncated_text += "\n\n[... Document tronquÃ© pour Ã©viter le dÃ©passement de tokens ...]"
        print(f"âš ï¸ Fallback: PDF tronquÃ© de {len(full_text)} Ã  {len(truncated_text)} caractÃ¨res")
        return truncated_text
    
    return full_text

# â”€â”€ ANCIEN SPEC_SUMMARY (garde en backup) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SPEC_SUMMARY = build_spec_summary_fallback()  # Garde Ã§a en commentaire pour backup