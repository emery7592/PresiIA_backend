"""
Centralise la configuration commune au module chat avec RAG.
"""
import os
import json
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from pypdf import PdfReader
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict, Tuple
from dataclasses import dataclass

# ‚îÄ‚îÄ Configuration g√©n√©rale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
load_dotenv(override=True)
BASE_DIR = Path(__file__).resolve().parent.parent
STATIC_DIR = BASE_DIR / "static" / "document"
PDF_PATH = STATIC_DIR / "specpense.pdf"

# ‚îÄ‚îÄ Clients externes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
openai_client = OpenAI()
PUSHOVER_USER = os.getenv("PUSHOVER_USER")
PUSHOVER_TOKEN = os.getenv("PUSHOVER_TOKEN")
PUSHOVER_URL = "https://api.pushover.net/1/messages.json"

# ‚îÄ‚îÄ Syst√®me RAG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@dataclass
class DocumentChunk:
    content: str
    page_number: int
    chunk_id: int

class SimpleRAG:
    """Version simplifi√©e du syst√®me RAG pour ton cas d'usage."""
    
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.chunks: List[DocumentChunk] = []
        self.embeddings = None
        self.index = None
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
    def extract_and_chunk_pdf(self, chunk_size: int = 400) -> List[DocumentChunk]:
        """Extrait et d√©coupe le PDF en chunks."""
        reader = PdfReader(self.pdf_path)
        chunks = []
        chunk_id = 0
        
        for page_num, page in enumerate(reader.pages):
            text = page.extract_text() or ""
            words = text.split()
            
            # D√©coupage en chunks
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i + chunk_size]
                chunk_content = " ".join(chunk_words)
                
                if len(chunk_content.strip()) > 50:  # √âviter les chunks trop petits
                    chunk = DocumentChunk(
                        content=chunk_content,
                        page_number=page_num + 1,
                        chunk_id=chunk_id
                    )
                    chunks.append(chunk)
                    chunk_id += 1
        
        self.chunks = chunks
        return chunks
    
    def build_embeddings(self):
        """G√©n√®re les embeddings pour tous les chunks."""
        print(f"üîÑ G√©n√©ration des embeddings pour {len(self.chunks)} chunks...")
        
        texts = [chunk.content for chunk in self.chunks]
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        
        # Cr√©ation de l'index FAISS
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        
        # Normalisation pour similarit√© cosinus
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        self.embeddings = embeddings
        print(f"‚úÖ Index FAISS cr√©√© avec {self.index.ntotal} vecteurs")
    
    def search_relevant_chunks(self, query: str, top_k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """Recherche les chunks les plus pertinents."""
        if self.index is None:
            raise ValueError("Index non cr√©√©. Appelez build_embeddings() d'abord.")
        
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(score)))
        
        return results
    
    def get_context_for_query(self, query: str, max_chars: int = 8000) -> str:
        """R√©cup√®re le contexte pertinent pour une query."""
        relevant_chunks = self.search_relevant_chunks(query, top_k=8)
        
        context_parts = []
        total_chars = 0
        
        for chunk, score in relevant_chunks:
            chunk_text = f"[Page {chunk.page_number}]\n{chunk.content}\n"
            
            if total_chars + len(chunk_text) <= max_chars:
                context_parts.append(chunk_text)
                total_chars += len(chunk_text)
            else:
                break
        
        return "\n---\n".join(context_parts)
    
    def save_index(self, base_path: str):
        """Sauvegarde l'index."""
        faiss.write_index(self.index, f"{base_path}.faiss")
        
        chunks_data = []
        for chunk in self.chunks:
            chunks_data.append({
                'content': chunk.content,
                'page_number': chunk.page_number,
                'chunk_id': chunk.chunk_id
            })
        
        with open(f"{base_path}_chunks.json", 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
    
    def load_index(self, base_path: str):
        """Charge un index sauvegard√©."""
        self.index = faiss.read_index(f"{base_path}.faiss")
        
        with open(f"{base_path}_chunks.json", 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        self.chunks = []
        for data in chunks_data:
            chunk = DocumentChunk(
                content=data['content'],
                page_number=data['page_number'],
                chunk_id=data['chunk_id']
            )
            self.chunks.append(chunk)

# ‚îÄ‚îÄ Initialisation du syst√®me RAG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def initialize_rag():
    """Initialise le syst√®me RAG (une seule fois)."""
    index_path = BASE_DIR / "rag_index"
    
    rag = SimpleRAG(str(PDF_PATH))
    
    # V√©rifier si l'index existe
    if (Path(f"{index_path}.faiss").exists() and 
        Path(f"{index_path}_chunks.json").exists()):
        print("üìö Chargement de l'index RAG existant...")
        rag.load_index(str(index_path))
        print(f"‚úÖ Index charg√©: {len(rag.chunks)} chunks disponibles")
    else:
        print("üîÑ Cr√©ation du nouvel index RAG...")
        rag.extract_and_chunk_pdf()
        rag.build_embeddings()
        rag.save_index(str(index_path))
        print(f"‚úÖ Index RAG cr√©√©: {len(rag.chunks)} chunks")
    
    return rag

# Initialisation globale
RAG_SYSTEM = initialize_rag()
#d√©sactiv√© pour le moment
#RAG_SYSTEM = None

# ‚îÄ‚îÄ Fonction de prompt intelligent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def get_system_prompt(user_query: str = "") -> str:
    """G√©n√®re un prompt avec contexte adaptatif."""
    name = "Ralph AI"
    
    # G√©n√©ration du contexte intelligent bas√© sur la query
    if user_query and user_query.strip():
        try:
            relevant_context = RAG_SYSTEM.get_context_for_query(user_query, max_chars=8000)
            print(f"üéØ Contexte RAG g√©n√©r√©: {len(relevant_context)} caract√®res")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur RAG: {e}")
            relevant_context = "Contexte non disponible"
    else:
        relevant_context = "Contexte sera g√©n√©r√© selon votre question"
    
    prompt = f"""Tu es {name}, assistant sp√©cialis√© dans la philosophie redpill masculine.

## R√àGLES ABSOLUES (√† respecter imp√©rativement) :

1. **R√©ponses bas√©es sur le document uniquement** :
   - Si la question est abord√©e dans specpense.pdf ‚Üí r√©ponds en utilisant EXCLUSIVEMENT le contenu du document
   - Cite les concepts et exemples du texte sans mentionner les num√©ros de page

2. **Questions hors sujet du document** :
   - Si c'est une question homme-femme/relations MAIS non couverte dans le document ‚Üí r√©ponds selon les principes redpill (responsabilit√© masculine, anti-victimisation, valeur personnelle, cadre relationnel)
   - Si ce n'est PAS une question homme-femme ‚Üí r√©ponds : "Cette question ne concerne pas les relations homme-femme. Je ne peux y r√©pondre."

3. **INTERDIT ABSOLU - Ne JAMAIS faire ceci** :
   ‚ùå Conseiller la compr√©hension √©motionnelle excessive de la femme
   ‚ùå Sugg√©rer que l'homme doit "faire plus d'efforts" pour une femme toxique
   ‚ùå Donner des r√©ponses "blue pill" du type "communiquez davantage", "soyez plus √† l'√©coute"
   ‚ùå Victimiser la femme ou d√©responsabiliser l'homme
   ‚ùå Encourager un homme √† rester dans une relation destructrice

4. **Ton et style** :
   - Reste direct, structur√© et masculin
   - Utilise des manifestes clairs avec titres en majuscules si pertinent
   - Sois ferme mais pas insultant envers le client
   - Termine par "ugh j'ai dit !" pour les r√©ponses longues bas√©es sur le document

5. **Langue de r√©ponse** :
   - R√©ponds dans la langue de la question (fran√ßais ‚Üí fran√ßais, anglais ‚Üí anglais, etc.)

## EXEMPLES DE BONNES vs MAUVAISES R√âPONSES :

‚ùå MAUVAIS (blue pill) :
"Votre femme vous critique ? Essayez de comprendre ses besoins √©motionnels..."

‚úÖ BON (redpill conforme au document) :
"Un homme fort √©tablit son cadre. Si elle critique constamment, c'est un test de dominance. Maintiens tes fronti√®res sans n√©gocier ton respect."

---

## Contexte pertinent du document :
{relevant_context}

---

R√©ponds maintenant √† la question du client en suivant ces r√®gles."""
    
    print(f"üìè Taille du prompt syst√®me : {len(prompt)} caract√®res (~{len(prompt)//4} tokens)")
    return prompt

# ‚îÄ‚îÄ Fonction de fallback (ton ancien syst√®me) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def build_spec_summary_fallback() -> str:
    """Fallback vers ton ancien syst√®me en cas de probl√®me."""
    reader = PdfReader(PDF_PATH)
    pages = [p.extract_text() or "" for p in reader.pages]
    full_text = "\n".join(pages)
    
    max_chars = 10000
    if len(full_text) > max_chars:
        truncated_text = full_text[:max_chars]
        truncated_text += "\n\n[... Document tronqu√© pour √©viter le d√©passement de tokens ...]"
        print(f"‚ö†Ô∏è Fallback: PDF tronqu√© de {len(full_text)} √† {len(truncated_text)} caract√®res")
        return truncated_text
    
    return full_text

# ‚îÄ‚îÄ ANCIEN SPEC_SUMMARY (garde en backup) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SPEC_SUMMARY = build_spec_summary_fallback()  # Garde √ßa en commentaire pour backup